# Scrapamoja ğŸ•·ï¸

> **A production-grade Python framework for building reliable, extensible web scrapers â€” with battle-tested resilience, stealth, and observability built in.**

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![Playwright](https://img.shields.io/badge/playwright-1.40+-green.svg)](https://playwright.dev/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)](https://github.com/TisoneK/scrapamoja/actions)

---

## What is Scrapamoja?

Scrapamoja blends the English word *Scrape* with the Swahili word *Pamoja* â€” meaning *together*. Scrape together. One scraper, many sites. One framework, many contributors.

It's also a quiet nod to *Moja*, Swahili for *one* â€” the idea that you shouldn't need a different tool for every site you want to scrape. One framework should be enough, and it should be good enough that anyone can extend it.

That philosophy shapes everything about how Scrapamoja is built. It's not a scraper â€” it's the infrastructure that makes scrapers reliable: handling anti-bot measures, selector drift, network failures, and browser resource leaks so you don't have to. New sites can be added by anyone, existing ones improved by the community, and the whole thing grows stronger the more people contribute to it.

*Scrape together. Build together.*

---

## Core Framework Capabilities

### ğŸ¯ Intelligent Selector Engine
The selector engine is the heart of Scrapamoja. Instead of brittle single-selector lookups, it uses a **multi-strategy approach** â€” CSS, XPath, and text-based selectors can all be defined for the same element. Each strategy is weighted, and the engine picks the best match with a confidence score. When a selector fails, it falls back gracefully rather than crashing. Selectors are defined in YAML, not hardcoded, making them easy to maintain without touching Python.

```
Site â†’ Sport â†’ Status â†’ Context â†’ Element
```

### ğŸ›¡ï¸ Resilience System
Built around the assumption that things will go wrong. Automatic retries with exponential backoff, failure classification (network vs. selector vs. parse errors), checkpoint-based recovery so long scrapes can resume, and a coordinator that ensures graceful shutdown even mid-scrape.

### ğŸ•µï¸ Stealth & Anti-Detection
A dedicated stealth module handles fingerprint randomization, human-like behavior simulation, consent popup handling, and proxy rotation. Sites that actively fight scrapers are manageable targets.

### ğŸ” Snapshot Debugging
When a scrape fails, Scrapamoja captures a full snapshot: the page HTML, a screenshot, structured logs, and selector resolution traces â€” all correlated by session ID. Debugging a failure means looking at exactly what the browser saw, not guessing.

### ğŸ“Š Telemetry & Observability
Structured JSON logging with correlation IDs, built-in metrics collection (execution time, success rates, selector confidence distributions), and alerting hooks. Production scrapers need production-grade monitoring.

### ğŸŒ Browser Lifecycle Management
Browser and page pooling, session state persistence, tab management, resource monitoring (memory, CPU), and corruption detection. Long-running scrapers won't leak memory or leave orphaned browser processes.

---

## Architecture

```
scrapamoja/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                   # Unified CLI entry point
â”‚   â”œâ”€â”€ sites/                    # Site implementations
â”‚   â”‚   â”œâ”€â”€ _template/            # Full-featured template for new sites
â”‚   â”‚   â”œâ”€â”€ base/                 # BaseSiteScraper, registry, DI container
â”‚   â”‚   â”œâ”€â”€ flashscore/           # FlashScore scraper (Basketball, Football)
â”‚   â”‚   â””â”€â”€ wikipedia/            # Wikipedia scraper
â”‚   â”œâ”€â”€ selectors/                # Selector engine (YAML-driven, multi-strategy)
â”‚   â”œâ”€â”€ browser/                  # Browser lifecycle, sessions, tab management
â”‚   â”œâ”€â”€ resilience/               # Retries, failure classification, checkpoints
â”‚   â”œâ”€â”€ stealth/                  # Anti-detection, fingerprinting, proxies
â”‚   â”œâ”€â”€ telemetry/                # Metrics, alerting, audit, reporting
â”‚   â”œâ”€â”€ navigation/               # Route planning and page discovery
â”‚   â”œâ”€â”€ extractor/                # Data extraction and transformation
â”‚   â”œâ”€â”€ observability/            # Structured logging and event system
â”‚   â””â”€â”€ interrupt_handling/       # Graceful shutdown and signal handling
â”œâ”€â”€ tests/                        # Unit, integration, performance, stealth tests
â”œâ”€â”€ docs/                         # Architecture docs, workflow guides, API reference
â”œâ”€â”€ specs/                        # Feature specs (17+ completed)
â”œâ”€â”€ examples/                     # Runnable examples
â””â”€â”€ scripts/                      # Migration and validation utilities
```

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.12 or higher
- 2GB RAM minimum (4GB recommended)
- Internet connection
- Git (for cloning)

### Installation

**Linux / macOS**
```bash
# 1. Clone repository
git clone https://github.com/TisoneK/scrapamoja.git
cd scrapamoja

# 2. Create virtual environment
python3 -m venv venv
source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Install Playwright browsers
playwright install chromium

# 5. Run your first scrape
python -m src.main flashscore scrape basketball live --limit 1
```

**Windows**
```bash
# 1. Clone repository
git clone https://github.com/TisoneK/scrapamoja.git
cd scrapamoja

# 2. Create virtual environment
python -m venv venv
venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Install Playwright browsers
playwright install chromium

# 5. Run your first scrape
python -m src.main flashscore scrape basketball live --limit 1
```

### First Results
```json
{
  "sport": "Basketball",
  "status": "live",
  "matches": [
    {
      "home_team": "Los Angeles Lakers",
      "away_team": "Boston Celtics",
      "score": "89-87",
      "time": "4th Quarter"
    }
  ],
  "total": 1
}
```

---

## ğŸ“¦ Installation

### Option 1: Standard Installation

**Linux / macOS**
```bash
git clone https://github.com/TisoneK/scrapamoja.git
cd scrapamoja
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
playwright install chromium
```

**Windows**
```bash
git clone https://github.com/TisoneK/scrapamoja.git
cd scrapamoja
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
playwright install chromium
```

### Option 2: Docker Installation
```dockerfile
FROM python:3.12-slim

WORKDIR /app
COPY . .

RUN pip install -r requirements.txt && \
    playwright install chromium --with-deps

ENTRYPOINT ["python", "-m", "src.main"]
```
```bash
docker build -t scrapamoja .
docker run scrapamoja flashscore scrape basketball live --limit 5
```

### Option 3: Development Installation

**Linux / macOS**
```bash
git clone https://github.com/TisoneK/scrapamoja.git
cd scrapamoja
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -e .
pre-commit install
pytest tests/
```

**Windows**
```bash
git clone https://github.com/TisoneK/scrapamoja.git
cd scrapamoja
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
pip install -e .
pre-commit install
pytest tests/
```

---

## Building a New Scraper

Scrapamoja ships with a full site template at `src/sites/_template/` â€” it's not a stub, it's a working skeleton with flows, processors, validators, config management, and component wiring already in place.

**1. Copy the template**
```bash
cp -r src/sites/_template src/sites/mysite
```

**2. Implement the scraper**
```python
from src.sites.base.site_scraper import BaseSiteScraper

class MySiteScraper(BaseSiteScraper):
    site_id = "mysite"
    site_name = "My Site"
    base_url = "https://example.com"
```

**3. Define selectors in YAML**
```yaml
# src/sites/mysite/selectors/extraction/listings.yaml
description: "Product listing items"
strategies:
  - type: "css"
    selector: ".product-card"
    weight: 1.0
  - type: "xpath"
    selector: "//div[@data-type='product']"
    weight: 0.8
```

**4. Register and run**
```python
# Add to SITE_CLIS in src/main.py
'mysite': ('src.sites.mysite.cli.main', 'MySiteCLI'),
```

---

## Supported Sites

| Site | Data | Sports/Topics | Status Types |
|------|------|---------------|--------------|
| [**FlashScore**](src/sites/flashscore/README.md) | Live scores, match stats, odds | Basketball, Football | Live, Finished, Scheduled |
| [**Wikipedia**](src/sites/wikipedia/README.md) | Article content, tables, references | Any | N/A |

Both are production implementations â€” the FlashScore scraper handles live match updates, status-aware extraction, and real-time polling. The Wikipedia scraper handles table parsing, multi-language articles, and reference extraction.

---

## Configuration

### Global config (`config.yaml`)

```yaml
browser:
  headless: true
  timeout: 30000
  viewport:
    width: 1920
    height: 1080

scraping:
  max_retries: 5
  retry_delay: 2.0
  rate_limit: 10  # requests per minute

logging:
  level: "INFO"
  structured: true
```

### CLI options

| Flag | Description |
|------|-------------|
| `--limit N` | Cap the number of results |
| `--output, -o FORMAT` | Output format: `json`, `csv`, `xml` |
| `--file, -f PATH` | Write output to file |
| `--headless / --no-headless` | Browser visibility (use `--no-headless` for debugging) |
| `--verbose, -v` | Detailed logs |
| `--quiet, -q` | Errors only |

---

## Testing

```bash
# Full suite
pytest tests/

# With coverage
pytest --cov=src tests/

# By category
pytest tests/unit/
pytest tests/integration/
pytest tests/performance/
pytest tests/stealth/
```

---

## Docker

```dockerfile
FROM python:3.12-slim

WORKDIR /app
COPY . .

RUN pip install -r requirements.txt && \
    playwright install chromium --with-deps

ENTRYPOINT ["python", "-m", "src.main"]
```

```bash
docker build -t scrapamoja .
docker run scrapamoja flashscore scrape basketball live --limit 5
```

---

## Documentation

| Doc | Description |
|-----|-------------|
| [features.md](docs/features.md) | Complete feature reference |
| [modular_template_guide.md](docs/modular_template_guide.md) | Guide to building new site scrapers |
| [snapshot_api_reference.md](docs/snapshot_api_reference.md) | Snapshot debugging system API |
| [yaml-configuration.md](docs/yaml-configuration.md) | YAML selector config reference |
| [browser-lifecycle-management.md](docs/browser-lifecycle-management.md) | Browser pooling and session management |
| [structured-logging-guide.md](docs/structured-logging-guide.md) | Logging and observability guide |

---

## Roadmap

### v1.2 (Q2 2026)
- Enhanced error recovery mechanisms
- GraphQL API integration
- Real-time WebSocket updates
- ML-based selector optimization

### v1.3 (Q3 2026)
- Tennis and Hockey sport support
- Enhanced telemetry dashboard
- Cloud deployment templates

### v2.0 (Q4 2026)
- ESPN support
- Multi-region scraping
- SaaS API offering
- Advanced analytics dashboard

---

## Contributing

1. Fork and clone the repo
2. Create a feature branch: `git checkout -b feature/my-feature`
3. Follow the existing code style (Black + Ruff)
4. Add tests for new functionality
5. Run `pytest tests/` and `ruff check src/` before submitting
6. Open a PR with a description of what changed and why

---

## License

MIT â€” see [LICENSE](LICENSE) for details.

---

**Built with â¤ï¸ by the Scrapamoja Team**
